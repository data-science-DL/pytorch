{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), \n",
    "                             (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    in_transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "to",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m content \u001b[39m=\u001b[39m load_image(\u001b[39m'\u001b[39;49m\u001b[39m./data/Images/City.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      2\u001b[0m style \u001b[39m=\u001b[39m load_image(\u001b[39m'\u001b[39m\u001b[39m./data/Images/StarryNight.jpg\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2.8\\lib\\site-packages\\PIL\\Image.py:519\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=511'>512</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=512'>513</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImage categories are deprecated and will be removed in Pillow 10 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=513'>514</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2023-07-01). Use is_animated instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=514'>515</a>\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=515'>516</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=516'>517</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=517'>518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m--> <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=518'>519</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: to"
     ]
    }
   ],
   "source": [
    "content = load_image('./data/Images/City.jpg').to(device)\n",
    "style = load_image('./data/Images/StarryNight.jpg').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    image = tensor.cpu().clone().detach().numpy()\n",
    "    image = image.squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
    "    image = image.clip(0, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m fig, (ax1, ax2) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m ax1\u001b[39m.\u001b[39mimshow(im_convert(content))\n\u001b[0;32m      3\u001b[0m ax1\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ax2\u001b[39m.\u001b[39mimshow(im_convert(style))\n",
      "\u001b[1;32mf:\\pytorch\\Pytorch for deeplearning and computer vision\\section5\\Style Transfer.ipynb Cell 8'\u001b[0m in \u001b[0;36mim_convert\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/pytorch/Pytorch%20for%20deeplearning%20and%20computer%20vision/section5/Style%20Transfer.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mim_convert\u001b[39m(tensor):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/pytorch/Pytorch%20for%20deeplearning%20and%20computer%20vision/section5/Style%20Transfer.ipynb#ch0000006?line=1'>2</a>\u001b[0m     image \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/pytorch/Pytorch%20for%20deeplearning%20and%20computer%20vision/section5/Style%20Transfer.ipynb#ch0000006?line=2'>3</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/pytorch/Pytorch%20for%20deeplearning%20and%20computer%20vision/section5/Style%20Transfer.ipynb#ch0000006?line=3'>4</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2.8\\lib\\site-packages\\PIL\\Image.py:519\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=511'>512</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=512'>513</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImage categories are deprecated and will be removed in Pillow 10 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=513'>514</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2023-07-01). Use is_animated instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=514'>515</a>\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=515'>516</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=516'>517</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=517'>518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[1;32m--> <a href='file:///c%3A/Users/rjsdu/anaconda3/envs/tf2.8/lib/site-packages/PIL/Image.py?line=518'>519</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: cpu"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax1.axis('off')\n",
    "ax2.imshow(im_convert(style))\n",
    "ax2.axis('off')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c200132d1e6ad0d08ee4f5a6aa73cd3fb0821d2e05edf12ce927a5bc1afafe0a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf2.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
